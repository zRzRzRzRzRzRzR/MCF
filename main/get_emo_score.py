import argparse
import asyncio
import json
import os
import re

from openai import OpenAI
from tqdm import tqdm

from utils import call_embedding, cosine_similarity, load_json, load_yaml_config


async def judge_similarity_with_llm(text_a, text_b, judge_config):
    """
    Use the LLM specified in judge_config to determine if text_a and text_b are similar, and return 0 or 1.

    Note: If the LLM does not return a number or provides additional explanations, it will be rated 0 points.
    Therefore, a LLM with stronger instruction-following capabilities and a larger weight should be used.
    In this work, we used the [GLM-4-Plus](https://arxiv.org/abs/2406.12793).

    Changing the LLM may affect the actual score evaluation, and a certain range of error is considered normal.
    """
    messages = [
        {
            "role": "system",
            "content": """
你是一个负责对比两个文本的评测器。
我会给你两个文本A和B，你需要判断它们语义是否相似，或者在描述一个相似的态度和事件。
如果语义基本一致或高度近似(意思相或者目标相同)，请输出1，否则输出0。只需要输出一个数字0或1，不要输出其他解释。

### 注意
- 输出只有一个数字，没有其他内容。
- 如果两个句子相似，则输出数字 1，否则输出数字 0。

### 输入示例

文本A: 我爱吃苹果
文本B: 我觉得苹果很好吃
请判断是否相似(0或1):

### 输出示例

1

""",
        },
        {
            "role": "user",
            "content": f"文本A: {text_a}\n文本B: {text_b}\n请判断是否相似(0或1):",
        },
    ]
    client = OpenAI(api_key=judge_config["api_key"], base_url=judge_config["base_url"])

    response = await asyncio.to_thread(
        client.chat.completions.create, model=judge_config["model"], messages=messages, temperature=0.0
    )
    content = response.choices[0].message.content.strip()
    val = int(content)
    return 1 if val == 1 else 0


async def match_event(gt_event_text, pred_events, embed_config, judge_config, event_threshold=0.3):
    """
    Calculate and match event similarity, and return the most similar event along with its similarity score.
    1. Determine the number of events and split the weight based on the quantity.
    2. Use embedding and LLM models to match the most similar event.
    Return two scores: embedding score and LLM score.

    Note: The default value of event_threshold=0.3 makes it easy to compare two events with some similarity. In this work, this is done to allow answers from the large model to enter the event pool for matching, as the events generated by the large model often have some differences compared to the ground truth (GT).
    This could lead to potential drawbacks, as it might not fully filter out irrelevant events, which often causes the LLM’s scores to be higher than expected.
    """
    if not pred_events:
        return None, 0.0, 0.0

    texts_to_embed = [gt_event_text] + [pe["event"] for pe in pred_events]
    embs = call_embedding(texts_to_embed, **embed_config)
    gt_emb = embs[0]
    pred_embs = embs[1:]

    best_sim = 0.0
    best_event = None
    for i, pred_ev in enumerate(pred_events):
        sim = cosine_similarity(gt_emb, pred_embs[i])
        if sim > best_sim:
            best_sim = sim
            best_event = pred_ev

    llm_sim = 0
    if best_event is not None:
        llm_sim = await judge_similarity_with_llm(gt_event_text, best_event["event"], judge_config)

    if best_sim < event_threshold and llm_sim < event_threshold:
        return None, 0.0, 0.0
    else:
        return best_event, best_sim, llm_sim


async def match_emotion(gt_emotion, pred_emotions, embed_config, judge_config):
    """
    Match sentiment, evaluate state, reason, and source_id.
    1. Use embedding model to calculate the similarity of reason.
    2. Use LLM model to determine reason similarity and return two scores.
    """
    gt_state = gt_emotion["state"]
    gt_reason = gt_emotion["reason"]
    gt_source_id = gt_emotion["source_id"]

    emo_res = {
        "gt_state": gt_state,
        "gt_reason": gt_reason,
        "gt_source_id": gt_source_id,
        "pred_state": None,
        "pred_source_id": None,
        "pred_reason": None,
        "state_score": 0,
        "source_id_score": 0,
        "reason_embed_score": 0,
        "reason_llm_score": 0,
    }
    if type(pred_emotions) is dict:
        pred_emotions = [pred_emotions]
    if len(pred_emotions) == 1 and pred_emotions[0] == {}:
        return emo_res, 0

    matched_idx = None

    max_length = 64
    texts_to_embed = [gt_reason] + [e["reason"] for e in pred_emotions]

    def chunk_texts(texts, max_length):
        return [texts[i : i + max_length] for i in range(0, len(texts), max_length)]

    chunked_texts = chunk_texts(texts_to_embed, max_length)
    embeddings = []

    # FIXME: ZhipuAI Embedding-3 API Only supports 64 groups of text, so we need to split the text into chunks.
    for chunk in chunked_texts:
        embs = call_embedding(chunk, **embed_config)
        embeddings.extend(embs)  # Append the embeddings from each chunk
    gt_emb = embeddings[0]
    pred_embs = embeddings[1:]

    best_sim = 0.0
    best_pair = None
    for i, pred_emo in enumerate(pred_emotions):
        sim = cosine_similarity(gt_emb, pred_embs[i])
        if sim > best_sim:
            best_sim = sim
            best_pair = (i, pred_emo)

    if best_pair:
        matched_idx, best_pred_emo = best_pair
        llm_reason_sim = await judge_similarity_with_llm(gt_reason, best_pred_emo["reason"], judge_config)

        # FIXME: llm_reason only has two cases, 0 and 1. This can be optimized in the future by adding embedding computation.
        if llm_reason_sim > 0:
            emo_res["reason_embed_score"] = best_sim
            emo_res["reason_llm_score"] = llm_reason_sim

        emo_res["pred_state"] = best_pred_emo.get("state", "")
        emo_res["pred_source_id"] = best_pred_emo.get("source_id", "")
        emo_res["pred_reason"] = best_pred_emo.get("reason", "")

        if gt_state == best_pred_emo["state"]:
            emo_res["state_score"] = 1

        if isinstance(gt_source_id, str):
            gt_source_ids = set(map(int, gt_source_id.replace("，", ",").split(",")))
        elif isinstance(gt_source_id, (list, set)):
            gt_source_ids = set(map(int, gt_source_id))
        else:
            gt_source_ids = {int(gt_source_id)}
        try:
            emo_res["source_id_score"] = 1 if best_pred_emo["source_id"] in gt_source_ids else 0
        except:
            emo_res["source_id_score"] = 0
    return emo_res, matched_idx


async def evaluate_chain(gt_data, pred_data, embed_config, judge_config, event_threshold=0.7):
    details = {}
    total_state_score = 0.0
    total_source_id_score = 0.0
    total_reason_llm_score = 0.0
    total_reason_mbed_score = 0.0
    total_possible_score = 0.0

    gt_roles = list(gt_data.keys())
    for role in gt_roles:
        gt_person_info = gt_data[role]
        pred_person_info = pred_data.get(role, {"events": []})
        gt_events = gt_person_info.get("events", [])
        pred_events = pred_person_info.get("events", [])

        num_gt_events = len(gt_events)

        event_details = []

        total_role_state_score = 0.0
        total_role_source_id_score = 0.0

        total_role_reason_llm_score = 0.0
        total_role_reason_embed_score = 0.0

        total_role_possible_score = 0.0

        for gt_ev in gt_events:
            gt_event_text = gt_ev["event"]
            best_pred_event, event_sim, llm_sim = await match_event(
                gt_event_text, pred_events, embed_config, judge_config, event_threshold
            )

            event_score = event_sim if best_pred_event else 0
            llm_event_score = llm_sim if best_pred_event else 0

            emo_matches = []
            num_emotions = len(gt_ev.get("emotions", []))

            if best_pred_event:
                for gt_em in gt_ev.get("emotions", []):
                    emo_res, _ = await match_emotion(
                        gt_em, best_pred_event.get("emotions", []), embed_config, judge_config
                    )
                    emo_matches.append(emo_res)

                    total_role_state_score += emo_res["state_score"] / num_emotions / num_gt_events
                    total_role_source_id_score += emo_res["source_id_score"] / num_emotions / num_gt_events
                    total_role_reason_llm_score += emo_res["reason_llm_score"] / num_emotions / num_gt_events
                    total_role_reason_embed_score += emo_res["reason_embed_score"] / num_emotions / num_gt_events

            event_details.append(
                {
                    "gt_event": gt_event_text,
                    "pred_event_matched": best_pred_event["event"] if best_pred_event else None,
                    "event_similarity": event_sim,
                    "event_score": event_score,
                    "llm_event_score": llm_event_score,
                    "emotions": emo_matches,
                }
            )

            total_role_possible_score += 1 / num_gt_events

        details[role] = {"events": event_details}

        total_state_score += total_role_state_score
        total_source_id_score += total_role_source_id_score
        total_reason_llm_score += total_role_reason_llm_score
        total_reason_mbed_score += total_role_reason_embed_score

        total_possible_score += total_role_possible_score

    total_reason_embed_score_percentage = (
        round((total_reason_mbed_score / total_possible_score) * 100, 2) if total_possible_score > 0 else 0.0
    )
    total_state_score_percentage = (
        round((total_state_score / total_possible_score) * 100, 2) if total_possible_score > 0 else 0.0
    )
    total_reason_llm_score_percentage = (
        round((total_reason_llm_score / total_possible_score) * 100, 2) if total_possible_score > 0 else 0.0
    )
    total_source_id_score_percentage = (
        round((total_source_id_score / total_possible_score) * 100, 2) if total_possible_score > 0 else 0.0
    )

    return {
        "total_score": {
            "total_reason_mbed_score": total_reason_mbed_score,
            "total_state_score": total_state_score,
            "total_source_id_score": total_source_id_score,
            "total_reason_llm_score": total_reason_llm_score,
            "total_possible_score": total_possible_score,
            "total_state_score_percentage": total_state_score_percentage,
            "total_source_id_score_percentage": total_source_id_score_percentage,
            "total_reason_llm_score_percentage": total_reason_llm_score_percentage,
            "total_reason_embed_score_percentage": total_reason_embed_score_percentage,
        },
        "details": details,
    }


async def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--gt_dir", type=str, required=True, help="GT folder (contains emo_event_x.json)")
    parser.add_argument(
        "--input_dir", type=str, required=True, help="Prediction folder (contains output_emotions_x.json)"
    )
    parser.add_argument(
        "--output_dir", type=str, default="evaluation_results", help="Output folder for evaluation results"
    )
    parser.add_argument("--config_path", type=str, default="config.yaml", help="Path to the YAML configuration file")
    parser.add_argument(
        "--embedding_model", type=str, default="zhipu", help="Embedding model name (corresponds to config.yaml)"
    )
    parser.add_argument("--llm_model", type=str, default="zhipu", help="LLM model name (corresponds to config.yaml)")
    parser.add_argument("--batch", type=int, default=4, help="Maximum number of concurrent processes")
    parser.add_argument("--event_threshold", type=float, default=0.3, help="Event matching threshold")
    args = parser.parse_args()

    os.makedirs(args.output_dir, exist_ok=True)
    embed_config = load_yaml_config(args.config_path, args.embedding_model, config_type="embed_config")
    judge_config = load_yaml_config(args.config_path, args.llm_model, config_type="llm_config")

    file_pairs = []
    for f in os.listdir(args.input_dir):
        m = re.match(r"emotions_(\d+)(?:_(events))?\.json", f)
        if m:
            idx = m.group(1)
            pred_file_path = os.path.join(args.input_dir, f)
            gt_file_path = os.path.join(args.gt_dir, f"chat_{idx}.json")
            out_file_path = os.path.join(args.output_dir, f"evaluation_{idx}.json")
            if not os.path.exists(gt_file_path) or os.path.exists(out_file_path):
                print(f"[WARN] Skipping {f}")
                continue
            file_pairs.append((gt_file_path, pred_file_path, out_file_path))

    file_pairs.sort(key=lambda x: int(re.search(r"emotions_(\d+)", x[1]).group(1)))
    sem = asyncio.Semaphore(args.batch)
    all_state_percentages = []
    all_source_id_percentages = []
    all_reason_llm_percentages = []
    all_reason_embed_percentages = []

    async def sem_wrapper(g_fp, p_fp, o_fp, pbar):
        async with sem:
            result = await evaluate_chain(
                load_json(g_fp),
                load_json(p_fp),
                embed_config,
                judge_config,
                event_threshold=args.event_threshold,
            )
            with open(o_fp, "w", encoding="utf-8") as f:
                json.dump(result, f, ensure_ascii=False, indent=2)

            total_score = result["total_score"]
            all_state_percentages.append(total_score["total_state_score_percentage"])
            all_source_id_percentages.append(total_score["total_source_id_score_percentage"])
            all_reason_llm_percentages.append(total_score["total_reason_llm_score_percentage"])
            all_reason_embed_percentages.append(total_score["total_reason_embed_score_percentage"])

            pbar.update(1)
            return result

    with tqdm(total=len(file_pairs), desc="Evaluating") as pbar:
        tasks = [asyncio.create_task(sem_wrapper(g, p, o, pbar)) for g, p, o in file_pairs]
        await asyncio.gather(*tasks)

    average_state_percentage = (
        round(sum(all_state_percentages) / len(all_state_percentages), 2) if all_state_percentages else 0.0
    )
    average_source_id_percentage = (
        round(sum(all_source_id_percentages) / len(all_source_id_percentages), 2) if all_source_id_percentages else 0.0
    )
    average_reason_llm_percentage = (
        round(sum(all_reason_llm_percentages) / len(all_reason_llm_percentages), 2)
        if all_reason_llm_percentages
        else 0.0
    )
    average_reason_embed_percentage = (
        round(sum(all_reason_embed_percentages) / len(all_reason_embed_percentages), 2)
        if all_reason_embed_percentages
        else 0.0
    )

    summary = {
        "average_score": {
            "total_state_score_percentage": average_state_percentage,
            "total_source_id_score_percentage": average_source_id_percentage,
            "total_reason_llm_score_percentage": average_reason_llm_percentage,
            "total_reason_embed_score_percentage": average_reason_embed_percentage,
        },
        "details": [],
    }

    for idx in range(len(all_state_percentages)):
        summary["details"].append(
            {
                "data_set": f"data_{idx + 1}",
                "total_state_score_percentage": all_state_percentages[idx],
                "total_source_id_score_percentage": all_source_id_percentages[idx],
                "total_reason_llm_score_percentage": all_reason_llm_percentages[idx],
                "total_reason_embed_score_percentage": all_reason_embed_percentages[idx],
            }
        )

    with open(os.path.join(args.output_dir, "summary.json"), "w", encoding="utf-8") as f:
        json.dump(summary, f, ensure_ascii=False, indent=2)

    print("[INFO] Summary has been saved to summary.json.")


if __name__ == "__main__":
    asyncio.run(main())
