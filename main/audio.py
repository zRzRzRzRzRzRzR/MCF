from modelscope import snapshot_download, AutoProcessor, Qwen2AudioForConditionalGeneration
import torch
import os
import json
from typing import Dict, List, Optional
import re
import librosa
import glob
from tqdm import tqdm
from datetime import datetime
import argparse
import traceback

class AudioAnalyzer:
    
    def __init__(self, model_dir: Optional[str] = None, cache_dir: str = '/autodl-tmp/models'):
        self.device = "cuda:0" if torch.cuda.is_available() else "cpu"

        if model_dir is None:
            model_dir = snapshot_download('Qwen/Qwen2-Audio-7B-Instruct', cache_dir=cache_dir)

        self.processor = AutoProcessor.from_pretrained(
            model_dir,
            trust_remote_code=True
        )

        self.model = Qwen2AudioForConditionalGeneration.from_pretrained(
            model_dir,
            device_map="auto",
            trust_remote_code=True,
            dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,
            low_cpu_mem_usage=True
        ).eval()

    def analyze_full_audio(self, audio_path: str) -> Dict:
        if not os.path.exists(audio_path):
            return {"error": f"éŸ³é¢‘æ–‡ä»¶ {audio_path} ä¸å­˜åœ¨"}

        try:
            print(f"Loading audio file: {os.path.basename(audio_path)}...")
            audio_data, sample_rate = librosa.load(
                audio_path,
                sr=self.processor.feature_extractor.sampling_rate
            )


            duration = len(audio_data) / sample_rate
            prompt_text = """è¯·å¯¹è¿™æ®µéŸ³é¢‘è¿›è¡Œå…¨é¢æ·±å…¥çš„åˆ†æï¼Œè¦æ±‚å¦‚ä¸‹ï¼š

ç‰¹åˆ«æ³¨æ„çš„æ˜¯ï¼Œå¦‚æœä½ ä¸èƒ½ç†è§£æˆ‘ä¼ é€’ç»™ä½ çš„éŸ³é¢‘ï¼Œè¯·è¯´"æ— æ³•ç†è§£éŸ³é¢‘å†…å®¹"ï¼Œè€Œä¸æ˜¯ç¼–é€ å†…å®¹ã€‚

## åˆ†æä»»åŠ¡ï¼š
1. **è¯´è¯äººè¯†åˆ«ä¸åŒºåˆ†**
   - è¯†åˆ«éŸ³é¢‘ä¸­å…±æœ‰å‡ ä½è¯´è¯äºº
   - ä¸ºæ¯ä½è¯´è¯äººåˆ†é…æ ‡è¯†ï¼ˆå¦‚ï¼šè¯´è¯äºº1ã€è¯´è¯äºº2ç­‰ï¼‰
   - æè¿°æ¯ä½è¯´è¯äººçš„å£°éŸ³ç‰¹å¾ï¼ˆæ€§åˆ«ã€å¹´é¾„æ®µä¼°è®¡ã€éŸ³è‰²ç‰¹ç‚¹ï¼‰

2. **æŒ‰è¯´è¯äººåˆ†æè¯­éŸ³ç‰¹å¾**
   å¯¹æ¯ä½è¯´è¯äººï¼Œè¯·åˆ†æä»¥ä¸‹å†…å®¹ï¼š
   
   a) **è¯­é€Ÿåˆ†æ**
      - å¹³å‡è¯­é€Ÿï¼ˆå¿«/ä¸­/æ…¢ï¼‰
      - è¯­é€Ÿå˜åŒ–æƒ…å†µï¼ˆæ˜¯å¦æœ‰æ˜æ˜¾çš„åŠ é€Ÿæˆ–å‡é€Ÿï¼‰
      - åœé¡¿æ¨¡å¼å’ŒèŠ‚å¥ç‰¹ç‚¹
   
   b) **è¯­è°ƒåˆ†æ**
      - åŸºæœ¬è¯­è°ƒç±»å‹ï¼ˆå¹³è°ƒ/å‡è°ƒ/é™è°ƒä¸»å¯¼ï¼‰
      - è¯­è°ƒå˜åŒ–èŒƒå›´ï¼ˆå•è°ƒ/é€‚ä¸­/ä¸°å¯Œï¼‰
      - æƒ…æ„Ÿè¡¨è¾¾å¼ºåº¦
   
   c) **éŸ³é«˜å˜åŒ–**
      - åŸºç¡€éŸ³é«˜æ°´å¹³ï¼ˆé«˜/ä¸­/ä½ï¼‰
      - éŸ³é«˜å˜åŒ–å¹…åº¦
      - éŸ³é«˜æ³¢åŠ¨æ¨¡å¼

3. **å†…å®¹è½¬å½•**
   - æŒ‰è¯´è¯äººåŒºåˆ†è½¬å½•å†…å®¹
   - æ ‡æ³¨æ¯æ®µè¯çš„è¯´è¯äºº

4. **æƒ…ç»ªçŠ¶æ€åˆ†æ**
   - åˆ†ææ¯ä½è¯´è¯äººçš„ä¸»è¦æƒ…ç»ªçŠ¶æ€
   - æƒ…ç»ªå˜åŒ–è½¨è¿¹

5. **äº¤äº’åˆ†æ**ï¼ˆå¦‚æœ‰å¤šäººï¼‰
   - è¯´è¯äººä¹‹é—´çš„äº’åŠ¨æ¨¡å¼
   - è¯è½®è½¬æ¢ç‰¹ç‚¹

## è¾“å‡ºæ ¼å¼è¦æ±‚ï¼š
è¯·æŒ‰ä»¥ä¸‹ç»“æ„åŒ–æ ¼å¼è¾“å‡ºï¼š

ã€éŸ³é¢‘æ¦‚è§ˆã€‘
- æ€»æ—¶é•¿ä¼°è®¡ï¼š
- è¯´è¯äººæ•°é‡ï¼š
- éŸ³é¢‘è´¨é‡ï¼š

ã€è¯´è¯äººåˆ†æã€‘
[è¯´è¯äºº1]
- å£°éŸ³ç‰¹å¾ï¼š
- è¯­é€Ÿï¼š
- è¯­è°ƒï¼š
- éŸ³é«˜ï¼š
- ä¸»è¦æƒ…ç»ªï¼š
- è½¬å½•å†…å®¹ï¼š

[è¯´è¯äºº2]
ï¼ˆåŒä¸Šæ ¼å¼ï¼‰

ã€äº¤äº’ç‰¹å¾ã€‘
- è¯è½®æ¨¡å¼ï¼š
- äº’åŠ¨ç‰¹ç‚¹ï¼š

è¯·ç¡®ä¿åˆ†æè¯¦å°½ä¸”æŒ‰è¯´è¯äººæ¸…æ™°åŒºåˆ†ã€‚è¯·å¸®æˆ‘åˆ†æè¿™æ®µéŸ³é¢‘ã€‚"""

            conversation = [
                {"role": "user", "content": [
                    {"type": "audio", "audio_data": audio_data},
                    {"type": "text", "text": prompt_text}
                ]}
            ]
            audios = []
            for message in conversation:
                if isinstance(message["content"], list):
                    for ele in message["content"]:
                        if ele["type"] == "audio":
                            audios.append(ele["audio_data"])
            text_input = self.processor.apply_chat_template(
                conversation,
                add_generation_prompt=True,
                tokenize=False
            )

            inputs = self.processor(
                text=text_input,
                audio=audios,
                return_tensors="pt",
                sampling_rate=sample_rate,
                padding=True
            )

            inputs = inputs.to("cuda")

            with torch.no_grad():
                generated_ids = self.model.generate(
                    **inputs,
                    max_new_tokens=4096,
                    temperature=0.3,
                    top_p=0.9,
                    do_sample=True,
                    pad_token_id=self.processor.tokenizer.eos_token_id,
                    eos_token_id=self.processor.tokenizer.eos_token_id,
                    repetition_penalty=1.05
                )
            generated_ids = [
                output_ids[len(input_ids):]
                for input_ids, output_ids in zip(inputs.input_ids, generated_ids)
            ]

            response = self.processor.batch_decode(
                generated_ids,
                skip_special_tokens=True,
                clean_up_tokenization_spaces=False
            )[0]

            parsed_result = self._parse_response(response)
            parsed_result["raw_response"] = response
            parsed_result["audio_info"] = {
                "duration": duration,
                "sample_rate": sample_rate,
                "file_path": audio_path
            }

            return parsed_result

        except Exception as e:
            traceback.print_exc()
            return {
                "error": f"å¤„ç†éŸ³é¢‘æ—¶å‡ºé”™: {str(e)}",
                "raw_response": "",
                "audio_info": {"file_path": audio_path}
            }

    def batch_analyze(self, audio_paths: List[str], output_dir: str = None) -> List[Dict]:
        results = []

        if output_dir and not os.path.exists(output_dir):
            os.makedirs(output_dir)

        for audio_path in tqdm(audio_paths, desc="Processing audio files"):
            print(f"\n{'='*60}")
            print(f"Processing: {os.path.basename(audio_path)}")
            print(f"{'='*60}")

            result = self.analyze_full_audio(audio_path)
            results.append(result)
            if output_dir:
                base_name = os.path.splitext(os.path.basename(audio_path))[0]

                txt_path = os.path.join(output_dir, f"{base_name}_analysis.txt")
                formatted_output = self.format_output(result)
                with open(txt_path, 'w', encoding='utf-8') as f:
                    f.write(formatted_output)
                print(f"âœ… Text report saved to: {txt_path}")

                json_path = os.path.join(output_dir, f"{base_name}_analysis.json")
                json_data = {k: v for k, v in result.items() if k != "raw_response"}
                with open(json_path, 'w', encoding='utf-8') as f:
                    json.dump(json_data, f, ensure_ascii=False, indent=2)
                print(f"âœ… JSON saved to: {json_path}")


        if output_dir and len(results) > 1:
            summary_path = os.path.join(output_dir, "batch_analysis_summary.txt")
            with open(summary_path, 'w', encoding='utf-8') as f:
                f.write("="*60)
                f.write("\næ‰¹é‡éŸ³é¢‘åˆ†ææ±‡æ€»æŠ¥å‘Š\n")
                f.write("="*60)
                f.write(f"\nå¤„ç†æ—¶é—´: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
                f.write(f"\nå¤„ç†æ–‡ä»¶æ€»æ•°: {len(audio_paths)}")
                f.write(f"\næˆåŠŸåˆ†æ: {len([r for r in results if 'error' not in r])}")
                f.write(f"\nåˆ†æå¤±è´¥: {len([r for r in results if 'error' in r])}")
                f.write("\n\næ–‡ä»¶åˆ—è¡¨:\n")
                for i, (path, result) in enumerate(zip(audio_paths, results), 1):
                    status = "âœ… æˆåŠŸ" if 'error' not in result else "âŒ å¤±è´¥"
                    f.write(f"{i}. {os.path.basename(path)} - {status}\n")
            print(f"âœ… Summary report saved to: {summary_path}")

        return results

    def _parse_response(self, response: str) -> Dict:
        result = {
            "overview": {},
            "speakers": [],
            "interaction": {},
            "raw_text": response
        }

        try:
            if "æ— æ³•ç†è§£éŸ³é¢‘å†…å®¹" in response:
                result["error"] = "æ¨¡å‹æ— æ³•ç†è§£éŸ³é¢‘å†…å®¹ï¼Œå¯èƒ½æ˜¯éŸ³é¢‘è´¨é‡é—®é¢˜æˆ–æ ¼å¼ä¸æ”¯æŒ"
                return result

            overview_match = re.search(r'ã€éŸ³é¢‘æ¦‚è§ˆã€‘(.*?)(?=ã€|$)', response, re.DOTALL)
            if overview_match:
                overview_text = overview_match.group(1)
                result["overview"] = self._extract_key_values(overview_text)

            # æå–æ¯ä¸ªè¯´è¯äººçš„ä¿¡æ¯
            speaker_pattern = r'\[è¯´è¯äºº(\d+)\](.*?)(?=\[è¯´è¯äºº|\ã€|$)'
            speaker_matches = re.finditer(speaker_pattern, response, re.DOTALL)

            for match in speaker_matches:
                speaker_id = match.group(1)
                speaker_text = match.group(2)

                speaker_info = {
                    "id": f"è¯´è¯äºº{speaker_id}",
                    "features": self._extract_speaker_features(speaker_text)
                }
                result["speakers"].append(speaker_info)

            interaction_match = re.search(r'ã€äº¤äº’ç‰¹å¾ã€‘(.*?)(?=ã€|$)', response, re.DOTALL)
            if interaction_match:
                interaction_text = interaction_match.group(1)
                result["interaction"] = self._extract_key_values(interaction_text)

        except Exception as e:
            print(f"è§£æå“åº”æ—¶å‡ºé”™: {e}")
            result["parse_error"] = str(e)

        return result

    def _extract_key_values(self, text: str) -> Dict:
        result = {}
        lines = text.strip().split('\n')
        for line in lines:
            if 'ï¼š' in line or ':' in line:
                parts = re.split('[ï¼š:]', line, 1)
                if len(parts) == 2:
                    key = parts[0].strip().replace('-', '').strip()
                    value = parts[1].strip()
                    if key and value:
                        result[key] = value
        return result

    def _extract_speaker_features(self, text: str) -> Dict:
        features = {}
        patterns = {
            "å£°éŸ³ç‰¹å¾": r'å£°éŸ³ç‰¹å¾[ï¼š:](.*?)(?=\n[-â€¢]|\n[^\n]*[ï¼š:]|$)',
            "è¯­é€Ÿ": r'è¯­é€Ÿ[ï¼š:](.*?)(?=\n[-â€¢]|\n[^\n]*[ï¼š:]|$)',
            "è¯­è°ƒ": r'è¯­è°ƒ[ï¼š:](.*?)(?=\n[-â€¢]|\n[^\n]*[ï¼š:]|$)',
            "éŸ³é«˜": r'éŸ³é«˜[ï¼š:](.*?)(?=\n[-â€¢]|\n[^\n]*[ï¼š:]|$)',
            "æƒ…ç»ª": r'(?:ä¸»è¦)?æƒ…ç»ª[ï¼š:](.*?)(?=\n[-â€¢]|\n[^\n]*[ï¼š:]|$)',
            "å†…å®¹": r'è½¬å½•å†…å®¹[ï¼š:](.*?)(?=\n[-â€¢]|\n[^\n]*[ï¼š:]|$)'
        }

        for key, pattern  in patterns.items():
            match = re.search(pattern, text, re.DOTALL)
            if match:
                    features[key] = match.group(1).strip()

            return features

    def format_output(self, analysis_result: Dict) -> str:
        if "error" in analysis_result:
            return f"é”™è¯¯: {analysis_result['error']}"

        output = []
        output.append("="*60)
        output.append("éŸ³é¢‘åˆ†ææŠ¥å‘Š")
        output.append("="*60)

        # è¾“å‡ºéŸ³é¢‘ä¿¡æ¯
        if analysis_result.get("audio_info"):
            info = analysis_result["audio_info"]
            output.append(f"\nã€éŸ³é¢‘æ–‡ä»¶ä¿¡æ¯ã€‘")
            output.append(f"  æ–‡ä»¶è·¯å¾„: {info.get('file_path', 'N/A')}")
            output.append(f"  éŸ³é¢‘æ—¶é•¿: {info.get('duration', 0):.2f}ç§’")
            output.append(f"  é‡‡æ ·ç‡: {info.get('sample_rate', 0)}Hz")

        # è¾“å‡ºæ¦‚è§ˆ
        if analysis_result.get("overview"):
            output.append("\nã€éŸ³é¢‘æ¦‚è§ˆã€‘")
            for key, value in analysis_result["overview"].items():
                output.append(f"  {key}: {value}")

        # è¾“å‡ºè¯´è¯äººåˆ†æ
        if analysis_result.get("speakers"):
            output.append("\nã€è¯´è¯äººè¯¦ç»†åˆ†æã€‘")
            for speaker in analysis_result["speakers"]:
                output.append(f"\n{'-'*40}")
                output.append(f"ğŸ“¢ {speaker['id']}")
                output.append(f"{'-'*40}")

                features = speaker.get("features", {})
                for key, value in features.items():
                    if value:
                        output.append(f"  â–ª {key}: {value}")

        # è¾“å‡ºäº¤äº’åˆ†æ
        if analysis_result.get("interaction") and analysis_result["interaction"]:
            output.append("\nã€äº¤äº’ç‰¹å¾åˆ†æã€‘")
            for key, value in analysis_result["interaction"].items():
                output.append(f"  {key}: {value}")

        output.append("\n" + "="*60)

        return "\n".join(output)


def main():
    parser = argparse.ArgumentParser(description='éŸ³é¢‘åˆ†æå·¥å…·')
    parser.add_argument('--audio_path', type=str, required=True, help='éŸ³é¢‘æ–‡ä»¶è·¯å¾„æˆ–åŒ…å«éŸ³é¢‘æ–‡ä»¶çš„ç›®å½•')
    parser.add_argument('--model_dir', type=str, default='', help='æ¨¡å‹ç›®å½•è·¯å¾„ï¼Œå¦‚æœä¸ºç©ºåˆ™è‡ªåŠ¨ä¸‹è½½')
    parser.add_argument('--output_dir', type=str, default='./audio_analysis_results', help='ç»“æœè¾“å‡ºç›®å½•ï¼Œé»˜è®¤ä¸º./audio_analysis_results')

    args = parser.parse_args()

    print("\n" + "="*60)
    print("="*60)

    # åˆå§‹åŒ–åˆ†æå™¨
    try:
        analyzer = AudioAnalyzer(model_dir=args.model_dir)
    except Exception as e:
        print(f"âŒ Failed to initialize analyzer: {e}")
        return

    # æ”¶é›†è¦å¤„ç†çš„éŸ³é¢‘æ–‡ä»¶
    audio_files = []

    if os.path.isfile(args.audio_path):
        # å•ä¸ªæ–‡ä»¶æ¨¡å¼
        audio_files = [args.audio_path]
    elif os.path.isdir(args.audio_path):
        # ç›®å½•æ¨¡å¼ - æ”¶é›†æ‰€æœ‰éŸ³é¢‘æ–‡ä»¶
        supported_formats = ['.mp3']
        for ext in supported_formats:
            audio_files.extend(glob.glob(os.path.join(args.audio_path, f'*{ext}')))
            audio_files.extend(glob.glob(os.path.join(args.audio_path, f'*{ext.upper()}')))

        if not audio_files:
            print(f"âŒ No supported audio files found in {args.audio_path}")
            return

        print(f"Found {len(audio_files)} audio files")
    else:
        print(f"âŒ Path {args.audio_path} is neither a file nor a directory")
        return

    if len(audio_files) > 1:
        print("File list:")
        for i, file in enumerate(audio_files, 1):
            print(f"  {i}. {os.path.basename(file)}")

    confirm = input("\nProceed with processing? (y/n): ")
    if confirm.lower() != 'y':
        print("Processing canceled")
        return
    print("-" * 60)

    results = analyzer.batch_analyze(audio_files, args.output_dir)

    # è¾“å‡ºæ±‡æ€»ä¿¡æ¯
    print("\n" + "="*60)
    print("Batch processing completed!")
    print("="*60)
    print(f"Total files: {len(audio_files)}")
    print(f"Succeeded: {len([r for r in results if 'error' not in r])}")
    print(f"Failed: {len([r for r in results if 'error' in r])}")

    if len([r for r in results if 'error' in r]) > 0:
        print("\nFailed files:")
        for path, result in zip(audio_files, results):
            if 'error' in result:
                print(f"  âŒ {os.path.basename(path)}: {result['error']}")

    print(f"\nAll results saved to: {args.output_dir}")


if __name__ == "__main__":
    main()